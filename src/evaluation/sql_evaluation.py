"""
FILE: sql_evaluation.py
STATUS: Active
RESPONSIBILITY: SQL and Hybrid query evaluation framework
LAST MAJOR UPDATE: 2026-02-08
MAINTAINER: Shahu
"""

from enum import Enum
from typing import Any

from pydantic import BaseModel, Field


class QueryType(str, Enum):
    """Type of query being evaluated."""

    SQL_ONLY = "sql_only"  # Pure statistical query (SQL tool only)
    CONTEXTUAL_ONLY = "contextual_only"  # Pure contextual query (vector only)
    HYBRID = "hybrid"  # Requires both SQL + vector


class SQLEvaluationTestCase(BaseModel):
    """Test case for SQL evaluation."""

    question: str = Field(min_length=1, description="User question")
    query_type: QueryType = Field(description="Query type (SQL/CONTEXTUAL/HYBRID)")
    expected_sql: str | None = Field(default=None, description="Expected SQL query (for SQL/HYBRID)")
    ground_truth_answer: str = Field(min_length=1, description="Correct answer")
    ground_truth_data: dict[str, Any] | None = Field(
        default=None, description="Expected SQL results (for verification)"
    )
    category: str = Field(description="Category (simple/complex/comparison/aggregation)")


class SQLExecutionResult(BaseModel):
    """Result of SQL query execution."""

    query_generated: str | None = Field(description="SQL query generated by LLM")
    query_executed: bool = Field(description="Whether query executed successfully")
    execution_error: str | None = Field(default=None, description="Execution error if any")
    results: list[dict[str, Any]] = Field(default_factory=list, description="Query results")
    formatted_results: str | None = Field(default=None, description="Formatted results for LLM")


class VectorRetrievalResult(BaseModel):
    """Result of vector search."""

    retrieved_contexts: list[str] = Field(default_factory=list, description="Retrieved chunks")
    retrieval_scores: list[float] = Field(default_factory=list, description="Similarity scores")
    sources: list[str] = Field(default_factory=list, description="Source files")


class HybridEvaluationSample(BaseModel):
    """Complete evaluation sample for hybrid queries."""

    user_input: str = Field(description="User question")
    query_type: QueryType = Field(description="Detected query type")

    # SQL component
    sql_result: SQLExecutionResult | None = Field(
        default=None, description="SQL execution result (if applicable)"
    )

    # Vector component
    vector_result: VectorRetrievalResult | None = Field(
        default=None, description="Vector retrieval result (if applicable)"
    )

    # Final response
    response: str = Field(description="Final generated answer")
    reference: str = Field(description="Ground truth answer")

    # Metadata
    category: str = Field(description="Test case category")


class SQLAccuracyMetrics(BaseModel):
    """Metrics for SQL component accuracy."""

    sql_syntax_correct: bool = Field(description="Generated SQL is syntactically valid")
    sql_semantic_correct: bool = Field(
        description="SQL query correctly answers the question"
    )
    results_accurate: bool = Field(description="Results match ground truth")
    execution_success: bool = Field(description="Query executed without errors")

    @property
    def overall_score(self) -> float:
        """Compute overall SQL accuracy score (0-1)."""
        scores = [
            self.sql_syntax_correct,
            self.sql_semantic_correct,
            self.results_accurate,
            self.execution_success,
        ]
        return sum(scores) / len(scores)


class HybridIntegrationMetrics(BaseModel):
    """Metrics for hybrid query integration quality."""

    sql_component_used: bool = Field(
        description="SQL results were used in final answer"
    )
    vector_component_used: bool = Field(
        description="Vector context was used in final answer"
    )
    components_blended: bool = Field(
        description="Both components integrated coherently (not just concatenated)"
    )
    answer_complete: bool = Field(
        description="Answer addresses both statistical and contextual aspects"
    )

    @property
    def integration_score(self) -> float:
        """Compute integration quality score (0-1)."""
        scores = [
            self.sql_component_used,
            self.vector_component_used,
            self.components_blended,
            self.answer_complete,
        ]
        return sum(scores) / len(scores)


class HybridEvaluationMetrics(BaseModel):
    """Complete metrics for hybrid query evaluation."""

    # SQL component (if applicable)
    sql_accuracy: SQLAccuracyMetrics | None = Field(
        default=None, description="SQL accuracy metrics"
    )

    # Vector component (if applicable)
    vector_faithfulness: float | None = Field(
        default=None, description="RAGAS faithfulness score"
    )
    vector_context_precision: float | None = Field(
        default=None, description="RAGAS context precision"
    )
    vector_context_recall: float | None = Field(
        default=None, description="RAGAS context recall"
    )

    # Integration metrics (for HYBRID queries)
    integration: HybridIntegrationMetrics | None = Field(
        default=None, description="Integration quality metrics"
    )

    # Overall metrics
    answer_relevancy: float = Field(description="Answer relevancy (RAGAS or custom)")
    answer_correctness: float = Field(
        description="Overall answer correctness (0-1, human or LLM-judged)"
    )

    @property
    def overall_score(self) -> float:
        """Compute overall evaluation score."""
        scores = []

        # SQL component (weight: 0.4 if present)
        if self.sql_accuracy:
            scores.append(self.sql_accuracy.overall_score * 0.4)

        # Vector component (weight: 0.4 if present)
        if self.vector_faithfulness is not None:
            vector_score = (
                self.vector_faithfulness * 0.5
                + (self.vector_context_precision or 0) * 0.25
                + (self.vector_context_recall or 0) * 0.25
            )
            scores.append(vector_score * 0.4)

        # Integration (weight: 0.2 for HYBRID queries)
        if self.integration:
            scores.append(self.integration.integration_score * 0.2)

        # Answer quality (always included, weight: 0.2-0.6 depending on components)
        answer_weight = 1.0 - sum([0.4 if self.sql_accuracy else 0,
                                   0.4 if self.vector_faithfulness else 0,
                                   0.2 if self.integration else 0])
        scores.append((self.answer_relevancy + self.answer_correctness) / 2 * answer_weight)

        return sum(scores) if scores else 0.0


class HybridEvaluationReport(BaseModel):
    """Complete evaluation report for SQL/Hybrid queries."""

    overall_metrics: HybridEvaluationMetrics
    category_results: list[dict[str, Any]] = Field(default_factory=list)
    sample_count: int = Field(ge=0)
    sql_query_count: int = Field(ge=0, description="Number of SQL queries evaluated")
    hybrid_query_count: int = Field(ge=0, description="Number of hybrid queries evaluated")
    contextual_query_count: int = Field(ge=0, description="Number of vector-only queries")
